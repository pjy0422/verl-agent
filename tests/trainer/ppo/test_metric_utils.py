# Copyright 2025 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Tests for the metric utilities in verl.trainer.ppo.metric_utils.
"""

import unittest
from unittest.mock import MagicMock, patch

import numpy as np
import torch

from verl.trainer.ppo.metric_utils import (
    bootstrap_metric,
    calc_maj_val,
    compute_data_metrics,
    compute_throughout_metrics,
    compute_timing_metrics,
    process_validation_metrics,
)

from verl.utils.metric import (
    reduce_metrics,
)


class TestReduceMetrics(unittest.TestCase):
    """Tests for the reduce_metrics function."""

    def test_reduce_metrics_basic(self):
        """Test that reduce_metrics correctly computes means."""
        metrics = {
            "loss": [1.0, 2.0, 3.0],
            "accuracy": [0.0, 0.5, 1.0],
        }
        result = reduce_metrics(metrics)
        
        self.assertEqual(result["loss"], 2.0)
        self.assertEqual(result["accuracy"], 0.5)
    
    def test_reduce_metrics_empty(self):
        """Test that reduce_metrics handles empty lists."""
        metrics = {
            "empty": [],
        }
        result = reduce_metrics(metrics)
        
        self.assertTrue(np.isnan(result["empty"]))
    
    def test_reduce_metrics_single_value(self):
        """Test that reduce_metrics works with single values."""
        metrics = {
            "single": [5.0],
        }
        result = reduce_metrics(metrics)
        
        self.assertEqual(result["single"], 5.0)


class TestComputeDataMetrics(unittest.TestCase):
    """Tests for the compute_data_metrics function."""
    
    def setUp(self):
        """Set up common test data."""
        # Create a mock DataProto object
        self.batch = MagicMock()
        self.batch.batch = {
            "token_level_scores": torch.tensor([[1.0, 2.0], [3.0, 4.0]]),
            "token_level_rewards": torch.tensor([[0.5, 1.0], [1.5, 2.0]]),
            "advantages": torch.tensor([[0.1, 0.2], [0.3, 0.4]]),
            "returns": torch.tensor([[1.1, 1.2], [1.3, 1.4]]),
            "responses": torch.zeros((2, 2)),  # 2 samples, 2 tokens each
            "attention_mask": torch.tensor([
                [1, 1, 1, 1],  # 2 prompt tokens, 2 response tokens
                [1, 1, 1, 1],
            ]),
            "values": torch.tensor([[0.9, 1.0], [1.1, 1.2]]),
        }
    
    def test_compute_data_metrics_with_critic(self):
        """Test compute_data_metrics with critic enabled."""
        metrics = compute_data_metrics(self.batch, use_critic=True)
        
        # Check that all expected metrics are present
        self.assertIn("critic/score/mean", metrics)
        self.assertIn("critic/rewards/mean", metrics)
        self.assertIn("critic/advantages/mean", metrics)
        self.assertIn("critic/returns/mean", metrics)
        self.assertIn("critic/values/mean", metrics)
        self.assertIn("critic/vf_explained_var", metrics)
        self.assertIn("response_length/mean", metrics)
        self.assertIn("prompt_length/mean", metrics)
        
        # Check some specific values
        self.assertAlmostEqual(metrics["critic/score/mean"], 5.0)  # Sum of token_level_scores
        self.assertAlmostEqual(metrics["critic/rewards/mean"], 2.5)  # Sum of token_level_rewards
    
    def test_compute_data_metrics_without_critic(self):
        """Test compute_data_metrics with critic disabled."""
        metrics = compute_data_metrics(self.batch, use_critic=False)
        
        # Check that critic-specific metrics are not present
        self.assertNotIn("critic/values/mean", metrics)
        self.assertNotIn("critic/vf_explained_var", metrics)
        
        # Check that other metrics are still present
        self.assertIn("critic/score/mean", metrics)
        self.assertIn("critic/rewards/mean", metrics)
        self.assertIn("response_length/mean", metrics)


class TestComputeTimingMetrics(unittest.TestCase):
    """Tests for the compute_timing_metrics function."""
    
    def setUp(self):
        """Set up common test data."""
        # Create a mock DataProto object
        self.batch = MagicMock()
        self.batch.batch = {
            "responses": torch.zeros((2, 3)),  # 2 samples, 3 response tokens each
            "attention_mask": torch.tensor([
                [1, 1, 1, 1, 1, 1],  # 3 prompt tokens, 3 response tokens
                [1, 1, 1, 1, 1, 1],
            ]),
        }
        
        # Mock the _compute_response_info function to return known values
        self.response_info = {
            "prompt_length": torch.tensor([3.0, 3.0]),
            "response_length": torch.tensor([3.0, 3.0]),
            "response_mask": torch.ones((2, 3)),
        }
    
    @patch("verl.trainer.ppo.metric_utils._compute_response_info")
    def test_compute_timing_metrics(self, mock_compute_response_info):
        """Test compute_timing_metrics with various timing data."""
        mock_compute_response_info.return_value = self.response_info
        
        timing_raw = {
            "gen": 0.5,  # 500ms
            "ref": 0.3,  # 300ms
            "values": 0.2,  # 200ms
        }
        
        metrics = compute_timing_metrics(self.batch, timing_raw)
        
        # Check raw timing metrics
        self.assertEqual(metrics["timing_s/gen"], 0.5)
        self.assertEqual(metrics["timing_s/ref"], 0.3)
        self.assertEqual(metrics["timing_s/values"], 0.2)
        
        # Check per-token timing metrics
        # gen uses only response tokens (6 tokens)
        self.assertAlmostEqual(metrics["timing_per_token_ms/gen"], 0.5 * 1000 / 6, places=5)
        
        # ref and values use all tokens (12 tokens)
        self.assertAlmostEqual(metrics["timing_per_token_ms/ref"], 0.3 * 1000 / 12, places=5)
        self.assertAlmostEqual(metrics["timing_per_token_ms/values"], 0.2 * 1000 / 12, places=5)


class TestComputeThroughputMetrics(unittest.TestCase):
    """Tests for the compute_throughout_metrics function."""
    
    def setUp(self):
        """Set up common test data."""
        # Create a mock DataProto object
        self.batch = MagicMock()
        self.batch.meta_info = {
            "global_token_num": [100, 200, 300],  # 600 tokens total
        }
    
    def test_compute_throughout_metrics(self):
        """Test compute_throughout_metrics with various timing data."""
        timing_raw = {
            "step": 2.0,  # 2 seconds per step
        }
        
        # Test with 1 GPU
        metrics = compute_throughout_metrics(self.batch, timing_raw, n_gpus=1)
        
        self.assertEqual(metrics["perf/total_num_tokens"], 600)
        self.assertEqual(metrics["perf/time_per_step"], 2.0)
        self.assertEqual(metrics["perf/throughput"], 600 / 2.0)  # 300 tokens/sec
        
        # Test with 2 GPUs
        metrics = compute_throughout_metrics(self.batch, timing_raw, n_gpus=2)
        
        self.assertEqual(metrics["perf/total_num_tokens"], 600)
        self.assertEqual(metrics["perf/time_per_step"], 2.0)
        self.assertEqual(metrics["perf/throughput"], 600 / (2.0 * 2))  # 150 tokens/sec/GPU


class TestBootstrapMetric(unittest.TestCase):
    """Tests for the bootstrap_metric function."""
    
    def test_bootstrap_metric_basic(self):
        """Test bootstrap_metric with simple data and functions."""
        data = [1, 2, 3, 4, 5]
        reduce_fns = [np.mean, np.max]
        
        # Use a fixed seed for reproducibility
        result = bootstrap_metric(data, subset_size=3, reduce_fns=reduce_fns, n_bootstrap=100, seed=42)
        
        # Check that we get two results (one for each reduce_fn)
        self.assertEqual(len(result), 2)
        
        # Each result should be a tuple of (mean, std)
        mean_result, max_result = result
        self.assertEqual(len(mean_result), 2)
        self.assertEqual(len(max_result), 2)
        
        # The mean of means should be close to the true mean (3.0)
        self.assertAlmostEqual(mean_result[0], 3.0, delta=0.3)
        
        # The mean of maxes should be close to the expected value for samples of size 3
        # For samples of size 3 from [1,2,3,4,5], the expected max is around 4.0-4.5
        self.assertGreater(max_result[0], 3.5)
        self.assertLess(max_result[0], 5.0)
    
    def test_bootstrap_metric_empty(self):
        """Test bootstrap_metric with empty data."""
        with self.assertRaises(ValueError):
            bootstrap_metric([], subset_size=1, reduce_fns=[np.mean])


class TestCalcMajVal(unittest.TestCase):
    """Tests for the calc_maj_val function."""
    
    def test_calc_maj_val_basic(self):
        """Test calc_maj_val with simple data."""
        data = [
            {"pred": "A", "val": 0.9},
            {"pred": "B", "val": 0.8},
            {"pred": "A", "val": 0.7},
        ]
        
        result = calc_maj_val(data, vote_key="pred", val_key="val")
        
        # "A" is the majority vote, so we should get the first "val" for "A"
        self.assertEqual(result, 0.9)
    
    def test_calc_maj_val_tie(self):
        """Test calc_maj_val with tied votes."""
        data = [
            {"pred": "A", "val": 0.9},
            {"pred": "B", "val": 0.8},
            {"pred": "B", "val": 0.7},
            {"pred": "A", "val": 0.6},
        ]
        
        # In case of a tie, the first key in sorted order wins
        # This depends on Python's dict implementation, but for this test
        # we just verify that one of the valid values is returned
        result = calc_maj_val(data, vote_key="pred", val_key="val")
        
        self.assertTrue(result in [0.9, 0.8])


class TestProcessValidationMetrics(unittest.TestCase):
    """Tests for the process_validation_metrics function."""
    
    def test_process_validation_metrics_basic(self):
        """Test process_validation_metrics with simple data."""
        data_sources = ["source1", "source1", "source2"]
        sample_inputs = ["prompt1", "prompt1", "prompt2"]
        infos_dict = {
            "score": [0.8, 0.9, 0.7],
        }
        
        result = process_validation_metrics(
            data_sources, sample_inputs, infos_dict, seed=42
        )
        
        # Check the structure of the result
        self.assertIn("source1", result)
        self.assertIn("source2", result)
        
        # Check that source1 has metrics for score
        self.assertIn("score", result["source1"])
        
        # Check that mean@2 is present for source1/score
        self.assertIn("mean@2", result["source1"]["score"])
        
        # Check the value of mean@2 for source1/score
        self.assertAlmostEqual(result["source1"]["score"]["mean@2"], 0.85)
    
    def test_process_validation_metrics_with_pred(self):
        """Test process_validation_metrics with prediction data."""
        data_sources = ["source1", "source1", "source1"]
        sample_inputs = ["prompt1", "prompt1", "prompt1"]
        infos_dict = {
            "score": [0.8, 0.9, 0.7],
            "pred": ["A", "B", "A"],
        }
        
        result = process_validation_metrics(
            data_sources, sample_inputs, infos_dict, seed=42
        )
        
        # Check that majority voting metrics are present
        self.assertIn("maj@2/mean", result["source1"]["score"])
        
        # For bootstrap with n=2, the majority vote could be either A or B
        # depending on the random sampling, so we don't check the exact value


class TestMultiTurnMetrics(unittest.TestCase):
    """Tests for the multi-turn metrics in compute_data_metrics.

    Scenario: 2 GRPO groups × 2 trajectories × 3 turns = 12 batch entries.

    Group g0:
      traj t0: turn_rewards=[0.2, 0.5, 0.95]  token_mask=[0,0,1,1,1,2,2]
      traj t1: turn_rewards=[0.1, 0.3, 0.4]   token_mask=[0,0,0,1,1,2]
    Group g1:
      traj t2: turn_rewards=[0.8, 0.92, 0.7]  token_mask=[0,0,1,1,2,2,2]
      traj t3: turn_rewards=[0.6, 0.85, 0.91] token_mask=[0,1,1,2,2]
    """

    def _make_batch(self):
        batch_size = 12  # 4 trajectories × 3 turns
        resp_len = 4
        prompt_len = 4

        # --- non_tensor_batch ---
        traj_uids = []
        uids = []
        step_indices = []
        turn_rewards_all = []
        turn_token_mask_all = []
        episode_rewards_all = []

        trajs = [
            ("t0", "g0", [0.2, 0.5, 0.95], [0, 0, 1, 1, 1, 2, 2]),
            ("t1", "g0", [0.1, 0.3, 0.4],  [0, 0, 0, 1, 1, 2]),
            ("t2", "g1", [0.8, 0.92, 0.7],  [0, 0, 1, 1, 2, 2, 2]),
            ("t3", "g1", [0.6, 0.85, 0.91], [0, 1, 1, 2, 2]),
        ]

        for traj_uid, uid, rewards, tmask in trajs:
            ep_reward = sum(rewards)
            for step in range(3):
                traj_uids.append(traj_uid)
                uids.append(uid)
                step_indices.append(step)
                turn_rewards_all.append(rewards)
                turn_token_mask_all.append(tmask)
                episode_rewards_all.append(ep_reward)

        non_tensor_batch = {
            "traj_uid": np.array(traj_uids, dtype=object),
            "uid": np.array(uids, dtype=object),
            "step_index": np.array(step_indices, dtype=object),
            "turn_rewards": np.array(turn_rewards_all, dtype=object),
            "turn_token_mask": np.array(turn_token_mask_all, dtype=object),
            "episode_rewards": np.array(episode_rewards_all, dtype=np.float32),
            "episode_lengths": np.array([3] * batch_size, dtype=np.float32),
            "tool_callings": np.array([0] * batch_size, dtype=np.float32),
        }

        # --- tensor batch (dummy values, just need the right shapes) ---
        tensor_batch = {
            "token_level_scores": torch.zeros(batch_size, resp_len),
            "token_level_rewards": torch.zeros(batch_size, resp_len),
            "advantages": torch.zeros(batch_size, resp_len),
            "returns": torch.zeros(batch_size, resp_len),
            "responses": torch.zeros(batch_size, resp_len),
            "attention_mask": torch.ones(batch_size, prompt_len + resp_len),
        }

        batch = MagicMock()
        batch.batch = tensor_batch
        batch.non_tensor_batch = non_tensor_batch
        return batch

    def test_per_turn_reward_stats(self):
        """turn_reward/turn_{t}/mean,std,max,min are correct."""
        batch = self._make_batch()
        metrics = compute_data_metrics(batch, use_critic=False)

        # Turn 1 (t=0): [0.2, 0.1, 0.8, 0.6]
        self.assertAlmostEqual(metrics["turn_reward/turn_1/mean"], 0.425, places=5)
        self.assertAlmostEqual(metrics["turn_reward/turn_1/max"], 0.8, places=5)
        self.assertAlmostEqual(metrics["turn_reward/turn_1/min"], 0.1, places=5)

        # Turn 2 (t=1): [0.5, 0.3, 0.92, 0.85]
        self.assertAlmostEqual(metrics["turn_reward/turn_2/mean"], 0.6425, places=5)
        self.assertAlmostEqual(metrics["turn_reward/turn_2/max"], 0.92, places=5)
        self.assertAlmostEqual(metrics["turn_reward/turn_2/min"], 0.3, places=5)

        # Turn 3 (t=2): [0.95, 0.4, 0.7, 0.91]
        self.assertAlmostEqual(metrics["turn_reward/turn_3/mean"], 0.74, places=5)
        self.assertAlmostEqual(metrics["turn_reward/turn_3/max"], 0.95, places=5)
        self.assertAlmostEqual(metrics["turn_reward/turn_3/min"], 0.4, places=5)

        # std check for turn 1: std([0.2, 0.1, 0.8, 0.6])
        expected_std = float(np.std([0.2, 0.1, 0.8, 0.6]))
        self.assertAlmostEqual(metrics["turn_reward/turn_1/std"], expected_std, places=5)

    def test_episode_success_rate(self):
        """episode/success_rate with threshold=0.9."""
        batch = self._make_batch()
        metrics = compute_data_metrics(batch, use_critic=False)

        # t0: max=0.95 >= 0.9 → success
        # t1: max=0.4  < 0.9  → fail
        # t2: max=0.92 >= 0.9 → success
        # t3: max=0.91 >= 0.9 → success
        # rate = 3/4 = 0.75
        self.assertAlmostEqual(metrics["episode/success_rate"], 0.75, places=5)

    def test_grpo_group_reward_std(self):
        """grpo/group_reward_std/* are correct."""
        batch = self._make_batch()
        metrics = compute_data_metrics(batch, use_critic=False)

        # g0: episode_rewards = [1.65, 0.8] → std
        g0_std = float(np.std([1.65, 0.8]))
        # g1: episode_rewards = [2.42, 2.36] → std
        g1_std = float(np.std([2.42, 2.36]))

        self.assertAlmostEqual(metrics["grpo/group_reward_std/mean"], (g0_std + g1_std) / 2, places=5)
        self.assertAlmostEqual(metrics["grpo/group_reward_std/min"], min(g0_std, g1_std), places=5)
        self.assertAlmostEqual(metrics["grpo/group_reward_std/max"], max(g0_std, g1_std), places=5)

    def test_per_turn_response_length(self):
        """turn_response_length/turn_{t}/mean,std are correct."""
        batch = self._make_batch()
        metrics = compute_data_metrics(batch, use_critic=False)

        # Turn 1 (t=0) token counts: t0→2, t1→3, t2→2, t3→1
        self.assertAlmostEqual(metrics["turn_response_length/turn_1/mean"], 2.0, places=5)
        expected_std = float(np.std([2, 3, 2, 1]))
        self.assertAlmostEqual(metrics["turn_response_length/turn_1/std"], expected_std, places=5)

        # Turn 2 (t=1) token counts: t0→3, t1→2, t2→2, t3→2
        self.assertAlmostEqual(metrics["turn_response_length/turn_2/mean"], 2.25, places=5)

        # Turn 3 (t=2) token counts: t0→2, t1→1, t2→3, t3→2
        self.assertAlmostEqual(metrics["turn_response_length/turn_3/mean"], 2.0, places=5)

    def test_reward_delta(self):
        """turn_reward/delta_last_first/mean,std are correct."""
        batch = self._make_batch()
        metrics = compute_data_metrics(batch, use_critic=False)

        # deltas: t0→0.75, t1→0.3, t2→-0.1, t3→0.31
        deltas = [0.95 - 0.2, 0.4 - 0.1, 0.7 - 0.8, 0.91 - 0.6]
        self.assertAlmostEqual(metrics["turn_reward/delta_last_first/mean"], float(np.mean(deltas)), places=5)
        self.assertAlmostEqual(metrics["turn_reward/delta_last_first/std"], float(np.std(deltas)), places=5)

    def test_best_of_n(self):
        """episode/reward/best_of_n is correct."""
        batch = self._make_batch()
        metrics = compute_data_metrics(batch, use_critic=False)

        # g0: max(1.65, 0.8) = 1.65
        # g1: max(2.42, 2.36) = 2.42
        self.assertAlmostEqual(metrics["episode/reward/best_of_n"], (1.65 + 2.42) / 2, places=5)

    def test_no_multiturn_metrics_without_turn_rewards(self):
        """Multi-turn metrics are absent when turn_rewards is not in batch."""
        batch = self._make_batch()
        del batch.non_tensor_batch["turn_rewards"]
        metrics = compute_data_metrics(batch, use_critic=False)

        self.assertNotIn("turn_reward/turn_1/mean", metrics)
        self.assertNotIn("episode/success_rate", metrics)
        self.assertNotIn("grpo/group_reward_std/mean", metrics)
        self.assertNotIn("episode/reward/best_of_n", metrics)

    def test_single_trajectory_per_group_skips_group_std(self):
        """grpo/group_reward_std is absent when each group has only 1 trajectory."""
        batch_size = 6  # 2 trajectories × 3 turns
        resp_len = 4
        prompt_len = 4

        traj_uids, uids, turn_rewards_all = [], [], []
        episode_rewards_all = []
        # Each trajectory in its own group
        for traj_uid, uid, rewards in [
            ("t0", "g0", [0.2, 0.5]),
            ("t1", "g1", [0.8, 0.9]),
        ]:
            for step in range(3):
                traj_uids.append(traj_uid)
                uids.append(uid)
                turn_rewards_all.append(rewards)
                episode_rewards_all.append(sum(rewards))

        batch = MagicMock()
        batch.batch = {
            "token_level_scores": torch.zeros(batch_size, resp_len),
            "token_level_rewards": torch.zeros(batch_size, resp_len),
            "advantages": torch.zeros(batch_size, resp_len),
            "returns": torch.zeros(batch_size, resp_len),
            "responses": torch.zeros(batch_size, resp_len),
            "attention_mask": torch.ones(batch_size, prompt_len + resp_len),
        }
        batch.non_tensor_batch = {
            "traj_uid": np.array(traj_uids, dtype=object),
            "uid": np.array(uids, dtype=object),
            "turn_rewards": np.array(turn_rewards_all, dtype=object),
            "episode_rewards": np.array(episode_rewards_all, dtype=np.float32),
            "episode_lengths": np.array([2] * batch_size, dtype=np.float32),
            "tool_callings": np.array([0] * batch_size, dtype=np.float32),
        }

        metrics = compute_data_metrics(batch, use_critic=False)
        self.assertNotIn("grpo/group_reward_std/mean", metrics)
        # best_of_n should still work (each group has one trajectory)
        self.assertIn("episode/reward/best_of_n", metrics)


if __name__ == "__main__":
    unittest.main()
